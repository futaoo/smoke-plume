{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":110319,"status":"ok","timestamp":1685446498121,"user":{"displayName":"Jiantao Wu","userId":"07523577744578165226"},"user_tz":-480},"id":"2fyQRQSgY8yJ","outputId":"7f7d6466-4bb1-4275-8d82-b4062c3d0c68"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting mahotas==1.4.12\n","  Downloading mahotas-1.4.12.tar.gz (1.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting rasterio\n","  Downloading rasterio-1.3.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mahotas==1.4.12) (1.22.4)\n","Collecting affine (from rasterio)\n","  Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n","Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from rasterio) (23.1.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from rasterio) (2022.12.7)\n","Requirement already satisfied: click\u003e=4.0 in /usr/local/lib/python3.10/dist-packages (from rasterio) (8.1.3)\n","Collecting cligj\u003e=0.5 (from rasterio)\n","  Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n","Collecting snuggs\u003e=1.4.1 (from rasterio)\n","  Downloading snuggs-1.4.7-py3-none-any.whl (5.4 kB)\n","Collecting click-plugins (from rasterio)\n","  Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from rasterio) (67.7.2)\n","Requirement already satisfied: pyparsing\u003e=2.1.6 in /usr/local/lib/python3.10/dist-packages (from snuggs\u003e=1.4.1-\u003erasterio) (3.0.9)\n","Building wheels for collected packages: mahotas\n","  Building wheel for mahotas (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for mahotas: filename=mahotas-1.4.12-cp310-cp310-linux_x86_64.whl size=5248769 sha256=20b2517273c098e08b43f72a0cbc4f040bd9b99841d4456bb07371ea3147d523\n","  Stored in directory: /root/.cache/pip/wheels/34/80/d8/cc6f405e77ffdc00b02717814bf19d8d19340eea63a585a8d3\n","Successfully built mahotas\n","Installing collected packages: snuggs, mahotas, cligj, click-plugins, affine, rasterio\n","Successfully installed affine-2.4.0 click-plugins-1.1.1 cligj-0.7.2 mahotas-1.4.12 rasterio-1.3.7 snuggs-1.4.7\n"]}],"source":["!pip install mahotas==1.4.12 rasterio"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32470,"status":"ok","timestamp":1685446535069,"user":{"displayName":"Jiantao Wu","userId":"07523577744578165226"},"user_tz":-480},"id":"X-h7slcOoVrn","outputId":"876c8732-6270-40b3-e11f-4b532907a942"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive/\n"]}],"source":["import os, sys\n","from google.colab import drive\n","drive.mount('/content/drive/')\n","os.chdir('/content/drive/My Drive/Colab Notebooks/smoke-plume-detection/')"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":434,"status":"ok","timestamp":1685446586839,"user":{"displayName":"Jiantao Wu","userId":"07523577744578165226"},"user_tz":-480},"id":"b_6Xa5Zko-It"},"outputs":[],"source":["import sys\n","sys.path.insert(0, '/content/drive/MyDrive/Colab Notebooks/smoke-plume-detection/IndustrialSmokePlumeDetection/segmentation')\n","sys.path.insert(0,'/content/drive/MyDrive/Colab Notebooks/smoke-plume-detection/pytorch-deeplab-xception')"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":189978,"status":"ok","timestamp":1685446779714,"user":{"displayName":"Jiantao Wu","userId":"07523577744578165226"},"user_tz":-480},"id":"pOmcEMjdodR-","outputId":"9eb91ca5-9594-4f64-e4d6-98d2087d5822"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u003cipython-input-4-c7b6818629d0\u003e:5: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n","  from tqdm.autonotebook import tqdm\n"]}],"source":["import numpy as np\n","import random\n","import torch\n","from torch import nn, optim\n","from tqdm.autonotebook import tqdm\n","from sklearn.metrics import accuracy_score\n","from torch.utils.data import DataLoader, random_split, SubsetRandomSampler\n","from torch.utils.tensorboard import SummaryWriter\n","import argparse\n","from sklearn.metrics import jaccard_score\n","\n","from data import create_dataset\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# prepare training and validation data loaders\n","data_all = create_dataset(\n","    datadir = '/content/drive/MyDrive/Colab Notebooks/smoke-plume-detection/data', # /path/to/image/data/\n","    seglabeldir = '/content/drive/MyDrive/Colab Notebooks/smoke-plume-detection/segmentation_labels', # /path/to/segmentation/labels/for/training/\n","    mult=1)\n","\n","len_all = len(data_all)\n","split_1 = (15*len_all) // 100\n","split_2 = 2*split_1\n","indices = list(range(len_all))\n","random.seed(9001)\n","random.shuffle(indices)\n","\n","test_sampler = SubsetRandomSampler(indices[:split_1])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":49},"id":"OEhquDTrpCWP"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2dcb5865a6dc485aa958b157acb42639","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/428 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["iou: 199 0.5636663542081247\n","F1: 0.8984198645598194\n","accuracy: 428 0.8948598130841121\n","mean area ratio: 428 0.915582913231845 0.060315780277511874\n"]}],"source":["import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","from torch.utils.data import DataLoader, random_split, RandomSampler\n","from tqdm.autonotebook import tqdm\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import jaccard_score\n","from sklearn.metrics import f1_score\n","\n","from network_ori import *\n","from model_unet import *\n","\n","np.random.seed(3)\n","torch.manual_seed(3)\n","\n","batch_size = 1 # 1 to create diagnostic images, any value otherwise\n","all_dl = DataLoader(data_all, batch_size=batch_size, sampler=test_sampler)\n","progress = tqdm(enumerate(all_dl), total=len(all_dl))\n","\n","# load model\n","device = torch.device('cpu')\n","\n","modelpaths={\n","    'u-net':'/content/drive/MyDrive/Colab Notebooks/smoke-plume-detection/models/unet_ep300_lr3e-01_bs60_mo0.7_111.model',\n","    'r2u-net':'/content/drive/MyDrive/Colab Notebooks/smoke-plume-detection/models/r2unet_ep300_lr5e-01_bs60_mo0.7_104.model',\n","    'attu-net':'/content/drive/MyDrive/Colab Notebooks/smoke-plume-detection/models/attunet_ep300_lr5e-01_bs60_mo0.7_116.model',\n","    'r2attu-net':'/content/drive/MyDrive/Colab Notebooks/smoke-plume-detection/models/r2attunet_ep300_lr5e-01_bs60_mo0.7_076.model',\n","    'dlabv3':'/content/drive/MyDrive/Colab Notebooks/smoke-plume-detection/models/dlabv3res_ep300_lr7e-01_bs60_mo0.7_147.model'\n","    }\n","\n","\n","# model = U_Net(img_ch=12, output_ch=1)\n","model = R2U_Net(img_ch=12, output_ch=1)\n","\n","\n","model.to(device)\n","model.load_state_dict(torch.load(\n","    '/content/drive/MyDrive/Colab Notebooks/smoke-plume-detection/models/r2unet_ep300_lr5e-01_bs60_mo0.7_104.model', map_location=torch.device('cpu')))\n","model.eval()\n","\n","# define loss function\n","loss_fn = nn.BCEWithLogitsLoss()\n","\n","# run through test data\n","all_ious = []\n","all_accs = []\n","all_true = []\n","all_pred = []\n","all_arearatios = []\n","for i, batch in progress:\n","    x, y = batch['img'].float().to(device), batch['fpt'].float().to(device)\n","    idx = batch['idx']\n","\n","    output = model(x)\n","\n","\n","    # obtain binary prediction map\n","    pred = np.zeros(output.shape)\n","    pred[output \u003e= 0] = 1\n","\n","\n","    # derive Iou score\n","    cropped_iou = []\n","    for j in range(y.shape[0]):\n","        z = jaccard_score(y[j].flatten().detach().numpy(),\n","                          pred[j][0].flatten())\n","        if (np.sum(pred[j][0]) != 0 and\n","            np.sum(y[j].detach().numpy()) != 0):\n","            cropped_iou.append(z)\n","\n","    all_ious = [*all_ious, *cropped_iou]\n","    \n","    # derive scalar binary labels on a per-image basis\n","    y_bin = np.array(np.sum(y.detach().numpy(),\n","                            axis=(1,2)) != 0).astype(int)\n","    prediction = np.array(np.sum(pred,\n","                               axis=(1,2,3)) != 0).astype(int)\n","\n","    all_true.append(y_bin)\n","    all_pred.append(prediction)\n","\n"," \n","    # derive image-wise accuracy for this batch\n","    all_accs.append(accuracy_score(y_bin, prediction))\n","\n","    # derive binary segmentation map from prediction\n","    output_binary = np.zeros(output.shape)\n","    output_binary[output.cpu().detach().numpy() \u003e= 0] = 1\n","\n","    # derive smoke areas\n","    area_pred = np.sum(output_binary, axis=(1,2,3))\n","    area_true = np.sum(y.cpu().detach().numpy(), axis=(1,2))\n","\n","    # derive smoke area ratios\n","    arearatios = []\n","    for k in range(len(area_pred)):\n","        if area_pred[k] == 0 and area_true[k] == 0:\n","            arearatios.append(1)\n","        elif area_true[k] == 0:\n","            arearatios.append(0)\n","        else:\n","            arearatios.append(area_pred[k]/area_true[k])\n","    all_arearatios = np.ravel([*all_arearatios, *arearatios])\n","\n","\n","    # if batch_size == 1:\n","\n","    #     if prediction == 1 and y_bin == 1:\n","    #         res = 'true_pos'\n","    #     elif prediction == 0 and y_bin == 0:\n","    #         res = 'true_neg'\n","    #     elif prediction == 0 and y_bin == 1:\n","    #         res = 'false_neg'\n","    #     elif prediction == 1 and y_bin == 0:\n","    #         res = 'false_pos'\n","\n","    #     # create plot\n","    #     f, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(1, 3))\n","\n","    #     # RGB plot\n","    #     ax1.imshow(0.2+1.5*(np.dstack([x[0][3], x[0][2], x[0][1]])-\n","    #                 np.min([x[0][3].numpy(),\n","    #                         x[0][2].numpy(),\n","    #                         x[0][1].numpy()]))/\n","    #                (np.max([x[0][3].numpy(),\n","    #                         x[0][2].numpy(),\n","    #                         x[0][1].numpy()])-\n","    #                 np.min([x[0][3].numpy(),\n","    #                         x[0][2].numpy(),\n","    #                         x[0][1].numpy()])),\n","    #                origin='upper')\n","    #     ax1.set_title({'true_pos': 'True Positive',\n","    #                    'true_neg': 'True Negative',\n","    #                    'false_pos': 'False Positive',\n","    #                    'false_neg': 'False Negative'}[res],\n","    #                   fontsize=8)\n","    #     ax1.set_xticks([])\n","    #     ax1.set_yticks([])\n","\n","    #     # false color plot\n","    #     ax2.imshow(0.2+(np.dstack([x[0][0], x[0][9], x[0][10]])-\n","    #                 np.min([x[0][0].numpy(),\n","    #                         x[0][9].numpy(),\n","    #                         x[0][10].numpy()]))/\n","    #                (np.max([x[0][0].numpy(),\n","    #                         x[0][9].numpy(),\n","    #                         x[0][10].numpy()])-\n","    #                 np.min([x[0][0].numpy(),\n","    #                         x[0][9].numpy(),\n","    #                         x[0][10].numpy()])),\n","    #                origin='upper')\n","\n","    #     ax2.set_xticks([])\n","    #     ax2.set_yticks([])\n","\n","    #     # segmentation ground-truth and prediction\n","    #     ax3.imshow(y[0], cmap='Reds', alpha=0.3)\n","    #     ax3.imshow(pred[0][0], cmap='Greens', alpha=0.3)\n","    #     ax3.set_xticks([])\n","    #     ax3.set_yticks([])\n","\n","    #     this_iou = jaccard_score(y[0].flatten().detach().numpy(),\n","    #                              pred[0][0].flatten())\n","    #     ax3.annotate(\"IoU={:.2f}\".format(this_iou), xy=(5,15), fontsize=8)\n","\n","    #     #segmentation probability distribution\n","       \n","    #     prob = torch.sigmoid(output)  #get the probability distribution\n","    #     ax4.imshow(prob[0][0].detach().numpy(), cmap = 'hot', vmin=0, vmax=1)\n","    #     ax4.set_xticks([])\n","    #     ax4.set_yticks([])\n","    #     ax4.annotate(\"Probability Map\")\n","    #     # print(prob[0][0].size())\n","    #     ##\n","\n","    #     f.subplots_adjust(0.05, 0.02, 0.95, 0.9, 0.05, 0.05)\n","\n","    #     plt.savefig('./eval-results/unet111/'+res+(os.path.split(batch['imgfile'][0])[1]).\\\n","    #                 replace('.tif', '_eval.png').replace(':', '_'),\n","    #                 tight_layout=True, dpi=200)\n","    #     plt.close()\n","\n","print('iou:', len(all_ious), np.average(all_ious))\n","print('F1:', f1_score(all_true, all_pred))\n","print('accuracy:', len(all_accs), np.average(all_accs))\n","print('mean area ratio:', len(all_arearatios), np.average(all_arearatios),\n","      np.std(all_arearatios)/np.sqrt(len(all_arearatios)-1))"]},{"cell_type":"markdown","metadata":{"id":"xak6j-cZ7CeF"},"source":["#### Evaluation for only DL models"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":41089,"status":"error","timestamp":1672133124519,"user":{"displayName":"Jiantao Wu","userId":"07523577744578165226"},"user_tz":-480},"id":"lUGvXkb2pP2q","outputId":"0c40ec3f-431e-4f40-bc0c-f63a89da8a9c"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"260812fc24074fad88c25b81bb044321","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/428 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"]},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-7-ca497e64f21d\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 81\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# obtain binary prediction map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-\u003e 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Colab Notebooks/smoke-plume-detection/./pytorch-deeplab-xception/modeling/deeplab.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 28\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_level_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maspp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_level_feat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-\u003e 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Colab Notebooks/smoke-plume-detection/./pytorch-deeplab-xception/modeling/backbone/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 119\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0mlow_level_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-\u003e 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-\u003e 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Colab Notebooks/smoke-plume-detection/./pytorch-deeplab-xception/modeling/backbone/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 34\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-\u003e 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u003e\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--\u003e 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]},{"data":{"image/png":"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\n","text/plain":["\u003cFigure size 360x288 with 20 Axes\u003e"]},"metadata":{},"output_type":"display_data"}],"source":["import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","from torch.utils.data import DataLoader, random_split, RandomSampler\n","from tqdm.autonotebook import tqdm\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import jaccard_score\n","\n","from network_ori import *\n","from model_unet import *\n","from modeling.deeplab import *\n","\n","np.random.seed(3)\n","torch.manual_seed(3)\n","\n","batch_size = 1 # 1 to create diagnostic images, any value otherwise\n","all_dl = DataLoader(data_all, batch_size=batch_size, sampler=test_sampler)\n","progress = tqdm(enumerate(all_dl), total=len(all_dl))\n","\n","# load model\n","device = torch.device('cpu')\n","\n","modelpaths={\n","    'u-net':'/content/drive/MyDrive/Colab Notebooks/smoke-plume-detection/models/unet_ep300_lr3e-01_bs60_mo0.7_111.model',\n","    'r2u-net':'/content/drive/MyDrive/Colab Notebooks/smoke-plume-detection/models/r2unet_ep300_lr5e-01_bs60_mo0.7_104.model',\n","    'attu-net':'/content/drive/MyDrive/Colab Notebooks/smoke-plume-detection/models/attunet_ep300_lr5e-01_bs60_mo0.7_116.model',\n","    'r2attu-net':'/content/drive/MyDrive/Colab Notebooks/smoke-plume-detection/models/r2attunet_ep300_lr5e-01_bs60_mo0.7_076.model',\n","    'dlabv3':'/content/drive/MyDrive/Colab Notebooks/smoke-plume-detection/models/dlabv3res_ep300_lr7e-01_bs60_mo0.7_147.model'\n","    }\n","\n","\n","models = {\n","    'u-net': U_Net(img_ch=12, output_ch=1),\n","    'r2u-net': R2U_Net(img_ch=12, output_ch=1),\n","    'attu-net': AttU_Net(img_ch=12, output_ch=1),\n","    'r2attu-net': R2AttU_Net(img_ch=12, output_ch=1),\n","    'dlabv3': DeepLab(num_classes=1, backbone='resnet')\n","\n","}\n","\n","\n","for model in list(models.keys()):\n","    models[model].to(device)\n","    models[model].load_state_dict(torch.load(modelpaths[model],\n","                                                        map_location=device))\n","    models[model].eval()\n","\n","\n","# define loss function\n","loss_fn = nn.BCEWithLogitsLoss()\n","\n","# run through test data\n","\n","\n","\n","all_ious = {}\n","for model in list(models.keys()):\n","    all_ious[model] = []\n","all_accs = {}\n","for model in list(models.keys()):\n","    all_accs[model] = []\n","all_arearatios = {}\n","for model in list(models.keys()):\n","    all_arearatios[model] = []\n","\n","\n","for i, batch in progress:\n","    x, y = batch['img'].float().to(device), batch['fpt'].float().to(device)\n","    idx = batch['idx']\n","\n","    output = {}\n","    pred = {}\n","\n","    # create plot\n","    f, axs = plt.subplots(4, len(list(models.keys())), figsize=(5, 4))\n","\n","    i = 0\n","\n","    for model in list(models.keys()):\n","        output = models[model](x)\n","\n","        # obtain binary prediction map\n","        pred = np.zeros(output.shape)\n","        pred[output\u003e=0] =1\n","\n","        # derive Iou score\n","        cropped_iou = []\n","        for j in range(y.shape[0]):\n","            z = jaccard_score(y[j].flatten().detach().numpy(),\n","                            pred[j][0].flatten())\n","            if (np.sum(pred[j][0]) != 0 and\n","                np.sum(y[j].detach().numpy()) != 0):\n","                cropped_iou.append(z)\n","\n","        all_ious[model] = [*all_ious[model], *cropped_iou]\n","\n","    \n","        # derive scalar binary labels on a per-image basis\n","        y_bin = np.array(np.sum(y.detach().numpy(),\n","                                axis=(1,2)) != 0).astype(int)\n","        prediction = np.array(np.sum(pred,\n","                                axis=(1,2,3)) != 0).astype(int)\n","\n","        # derive image-wise accuracy for this batch\n","        all_accs[model].append(accuracy_score(y_bin, prediction))\n","\n","        # derive binary segmentation map from prediction\n","        output_binary = np.zeros(output.shape)\n","        output_binary[output.cpu().detach().numpy() \u003e= 0] = 1\n","\n","        # derive smoke areas\n","        area_pred = np.sum(output_binary, axis=(1,2,3))\n","        area_true = np.sum(y.cpu().detach().numpy(), axis=(1,2))\n","\n","        # derive smoke area ratios\n","        arearatios = []\n","        for k in range(len(area_pred)):\n","            if area_pred[k] == 0 and area_true[k] == 0:\n","                arearatios.append(1)\n","            elif area_true[k] == 0:\n","                arearatios.append(0)\n","            else:\n","                arearatios.append(area_pred[k]/area_true[k])\n","        all_arearatios[model] = np.ravel([*all_arearatios[model], *arearatios])\n","\n","\n","        if batch_size == 1:\n","\n","            if prediction == 1 and y_bin == 1:\n","                res = 'true_pos'\n","            elif prediction == 0 and y_bin == 0:\n","                res = 'true_neg'\n","            elif prediction == 0 and y_bin == 1:\n","                res = 'false_neg'\n","            elif prediction == 1 and y_bin == 0:\n","                res = 'false_pos'\n","\n","\n","        # RGB plot\n","        axs[0,i].imshow(0.2+1.5*(np.dstack([x[0][3], x[0][2], x[0][1]])-\n","                    np.min([x[0][3].numpy(),\n","                            x[0][2].numpy(),\n","                            x[0][1].numpy()]))/\n","                   (np.max([x[0][3].numpy(),\n","                            x[0][2].numpy(),\n","                            x[0][1].numpy()])-\n","                    np.min([x[0][3].numpy(),\n","                            x[0][2].numpy(),\n","                            x[0][1].numpy()])),\n","                   origin='upper')\n","        axs[0,i].set_title({'true_pos': 'True Positive',\n","                       'true_neg': 'True Negative',\n","                       'false_pos': 'False Positive',\n","                       'false_neg': 'False Negative'}[res],\n","                      fontsize=8)\n","        axs[0,i].set_xticks([])\n","        axs[0,i].set_yticks([])\n","\n","        # false color plot\n","        axs[1,i].imshow(0.2+(np.dstack([x[0][0], x[0][9], x[0][10]])-\n","                    np.min([x[0][0].numpy(),\n","                            x[0][9].numpy(),\n","                            x[0][10].numpy()]))/\n","                   (np.max([x[0][0].numpy(),\n","                            x[0][9].numpy(),\n","                            x[0][10].numpy()])-\n","                    np.min([x[0][0].numpy(),\n","                            x[0][9].numpy(),\n","                            x[0][10].numpy()])),\n","                   origin='upper')\n","\n","        axs[1,i].set_xticks([])\n","        axs[1,i].set_yticks([])\n","\n","        # segmentation ground-truth and prediction\n","        axs[2,i].imshow(y[0], cmap='Reds', alpha=0.3)\n","        axs[2,i].imshow(pred[0][0], cmap='Greens', alpha=0.3)\n","        axs[2,i].set_xticks([])\n","        axs[2,i].set_yticks([])\n","\n","        this_iou = jaccard_score(y[0].flatten().detach().numpy(),\n","                                 pred[0][0].flatten())\n","        axs[2,i].annotate(\"IoU={:.2f}\".format(this_iou), xy=(5,15), fontsize=8)\n","\n","        #segmentation probability distribution\n","       \n","        prob = torch.sigmoid(output)  #get the probability distribution\n","        axs[3,i].imshow(prob[0][0].detach().numpy(), cmap = 'hot', vmin=0, vmax=1)\n","        axs[3,i].set_xticks([])\n","        axs[3,i].set_yticks([])\n","        axs[3,i].set_xlabel(model)\n","        # axs[3,i].annotate(\"Probability@\"+ model, xy=(5,15))\n","        # print(prob[0][0].size())\n","        ##\n","        i+=1\n","\n","    # f.subplots_adjust(0.05, 0.02, 0.95, 0.9, 0.05, 0.05)\n","\n","    plt.savefig('./eval-results/union/'+res+(os.path.split(batch['imgfile'][0])[1]).\\\n","                replace('.tif', '_eval.png').replace(':', '_'),\n","                tight_layout=True, dpi=200)\n","    plt.close()\n","\n","# print('iou:', len(all_ious), np.average(all_ious))\n","# print('accuracy:', len(all_accs), np.average(all_accs))\n","# print('mean area ratio:', len(all_arearatios), np.average(all_arearatios),\n","#       np.std(all_arearatios)/np.sqrt(len(all_arearatios)-1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"edvV36tS_W7w"},"outputs":[],"source":["# load model\n","device = torch.device('cpu')\n","\n","modelpaths={\n","    'u-net':'/content/drive/MyDrive/Colab Notebooks/smoke-plume-detection/models/unet_ep300_lr3e-01_bs60_mo0.7_111.model',\n","    'r2u-net':'/content/drive/MyDrive/Colab Notebooks/smoke-plume-detection/models/r2unet_ep300_lr5e-01_bs60_mo0.7_104.model',\n","    'attu-net':'/content/drive/MyDrive/Colab Notebooks/smoke-plume-detection/models/attunet_ep300_lr5e-01_bs60_mo0.7_116.model'\n","    }\n","\n","\n","models = {\n","    'u-net': U_Net(img_ch=12, output_ch=1),\n","    'r2u-net': R2U_Net(img_ch=12, output_ch=1),\n","    'attu-net': AttU_Net(img_ch=12, output_ch=1)\n","\n","}\n","\n","\n","for model in list(models.keys()):\n","    models[model].to(device)\n","    models[model].load_state_dict(torch.load(modelpaths[model],\n","                                                        map_location=device))\n","    models[model].eval()\n"]},{"cell_type":"markdown","metadata":{"id":"a3-Kh1CUBIgx"},"source":["#### Evaluation for DL + ML models"]},{"cell_type":"markdown","metadata":{"id":"OwwBXvdLHaje"},"source":["##### ML functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-WWNSh0YHtC_"},"outputs":[],"source":["!pip install numpy --upgrade \u0026\u0026 pip install mahotas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9JKzhNMkKS6r"},"outputs":[],"source":["import sys\n","sys.path.insert(0, '/content/drive/MyDrive/Colab Notebooks/smoke-plume-detection/IndustrialSmokePlumeDetection/segmentation')\n","sys.path.insert(0,'/content/drive/MyDrive/Colab Notebooks/smoke-plume-detection/pytorch-deeplab-xception')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xv2TRI6eHFrC"},"outputs":[],"source":["import cv2\n","import numpy as np\n","import pylab as plt\n","from glob import glob\n","import argparse\n","import os\n","import progressbar\n","import pickle as pkl\n","from numpy.lib import stride_tricks\n","from skimage import feature\n","from sklearn import metrics\n","from sklearn.model_selection import train_test_split\n","import time\n","import mahotas as mt\n","\n","def check_args(args):\n","\n","    if not os.path.exists(args.image_dir):\n","        raise ValueError(\"Image directory does not exist\")\n","\n","    if not os.path.exists(args.label_dir):\n","        raise ValueError(\"Label directory does not exist\")\n","\n","    if args.classifier != \"SVM\" and args.classifier != \"RF\" and args.classifier != \"GBC\":\n","        raise ValueError(\"Classifier must be either SVM, RF or GBC\")\n","\n","    if args.output_model.split('.')[-1] != \"p\":\n","        raise ValueError(\"Model extension must be .p\")\n","\n","    return args\n","\n","def parse_args():\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\"-i\", \"--image_dir\" , help=\"Path to images\", required=True)\n","    parser.add_argument(\"-l\", \"--label_dir\", help=\"Path to labels\", required=True)\n","    parser.add_argument(\"-c\", \"--classifier\", help=\"Classification model to use\", required = True)\n","    parser.add_argument(\"-o\", \"--output_model\", help=\"Path to save model. Must end in .p\", required = True)\n","    args = parser.parse_args()\n","    return check_args(args)\n","\n","def read_data(image_dir, label_dir):\n","\n","    print ('[INFO] Reading image data.')\n","\n","    filelist = glob(os.path.join(image_dir, '*.jpg'))\n","    image_list = []\n","    label_list = []\n","\n","    for file in filelist:\n","\n","        image_list.append(cv2.imread(file, 1))\n","        label_list.append(cv2.imread(os.path.join(label_dir, os.path.basename(file).split('.')[0]+'.png'), 0))\n","\n","    return image_list, label_list\n","\n","def subsample(features, labels, low, high, sample_size):\n","\n","    idx = np.random.randint(low, high, sample_size)\n","\n","    return features[idx], labels[idx]\n","\n","def subsample_idx(low, high, sample_size):\n","\n","    return np.random.randint(low,high,sample_size)\n","\n","def calc_haralick(roi):\n","\n","    feature_vec = []\n","\n","    texture_features = mt.features.haralick(roi)\n","    mean_ht = texture_features.mean(axis=0)\n","\n","    [feature_vec.append(i) for i in mean_ht[0:9]]\n","\n","    return np.array(feature_vec)\n","\n","def harlick_features(img, h_neigh, ss_idx):\n","\n","    print ('[INFO] Computing haralick features.')\n","    size = h_neigh\n","    shape = (img.shape[0] - size + 1, img.shape[1] - size + 1, size, size)\n","    strides = 2 * img.strides\n","    patches = stride_tricks.as_strided(img, shape=shape, strides=strides)\n","    patches = patches.reshape(-1, size, size)\n","\n","    if len(ss_idx) == 0 :\n","        bar = progressbar.ProgressBar(maxval=len(patches), \\\n","        widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n","    else:\n","        bar = progressbar.ProgressBar(maxval=len(ss_idx), \\\n","        widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n","\n","    bar.start()\n","\n","    h_features = []\n","\n","    if len(ss_idx) == 0:\n","        for i, p in enumerate(patches):\n","            bar.update(i+1)\n","            h_features.append(calc_haralick(p))\n","    else:\n","        for i, p in enumerate(patches[ss_idx]):\n","            bar.update(i+1)\n","            h_features.append(calc_haralick(p))\n","\n","    #h_features = [calc_haralick(p) for p in patches[ss_idx]]\n","\n","    return np.array(h_features)\n","\n","def create_binary_pattern(img, p, r):\n","\n","    print ('[INFO] Computing local binary pattern features.')\n","    lbp = feature.local_binary_pattern(img, p, r)\n","    return (lbp-np.min(lbp))/(np.max(lbp)-np.min(lbp)) * 255\n","\n","def create_features(img, img_gray, label, train=True):\n","\n","    lbp_radius = 24 # local binary pattern neighbourhood\n","    h_neigh = 11 # haralick neighbourhood\n","    num_examples = 100 # number of examples per image to use for training model\n","\n","    lbp_points = lbp_radius*8\n","    h_ind = int((h_neigh - 1)/ 2)\n","\n","    feature_img = np.zeros((img.shape[0],img.shape[1],13))\n","    feature_img[:,:,:12] = img\n","    img = None\n","    feature_img[:,:,12] = create_binary_pattern(img_gray, lbp_points, lbp_radius)\n","    feature_img = feature_img[h_ind:-h_ind, h_ind:-h_ind]\n","    features = feature_img.reshape(feature_img.shape[0]*feature_img.shape[1], feature_img.shape[2])\n","\n","    if train == True:\n","        ss_idx = subsample_idx(0, features.shape[0], num_examples)\n","        features = features[ss_idx]\n","    else:\n","        ss_idx = []\n","\n","    h_features = harlick_features(img_gray, h_neigh, ss_idx)\n","    features = np.hstack((features, h_features))\n","\n","    if train == True:\n","\n","        label = label[h_ind:-h_ind, h_ind:-h_ind]\n","        labels = label.reshape(label.shape[0]*label.shape[1], 1)\n","        labels = labels[ss_idx]\n","    else:\n","        labels = None\n","\n","    return features, labels\n","\n","def get_int_rgb(np_img):\n","    rgb_img =  (np.dstack([np_img[3], np_img[2], np_img[1]])-\n","                        np.min([np_img[3],\n","                                np_img[2],\n","                                np_img[1]]))/ \\\n","                    (np.max([np_img[3],\n","                                np_img[2],\n","                                np_img[1]])-\n","                        np.min([np_img[3],\n","                                np_img[2],\n","                                np_img[1]]))\n","    rgb_img = (rgb_img * 255).round().astype(np.uint8)\n","    full_img = np.moveaxis(np_img,0,-1)\n","    return rgb_img, full_img\n","    \n","\n","def create_training_dataset(image_list, label_list):\n","\n","    print ('[INFO] Creating training dataset on %d image(s).' %len(image_list))\n","\n","    X = []\n","    y = []\n","\n","    for i, img in enumerate(image_list):\n","        \n","        rgb_img, full_img = get_int_rgb(img)\n","        img_gray = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2GRAY)\n","        features, labels = create_features(full_img, img_gray, label_list[i])\n","        X.append(features)\n","        y.append(labels)\n","\n","    X = np.array(X)\n","    X = X.reshape(X.shape[0]*X.shape[1], X.shape[2])\n","    y = np.array(y)\n","    y = y.reshape(y.shape[0]*y.shape[1], y.shape[2]).ravel()\n","\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","    print ('[INFO] Feature vector size:', X_train.shape)\n","\n","    return X_train, X_test, y_train, y_test\n","\n","def train_model(X, y, classifier):\n","\n","    if classifier == \"SVM\":\n","        from sklearn.svm import SVC\n","        print ('[INFO] Training Support Vector Machine model.')\n","        model = SVC()\n","        model.fit(X, y)\n","    elif classifier == \"RF\":\n","        from sklearn.ensemble import RandomForestClassifier\n","        print ('[INFO] Training Random Forest model.')\n","        model = RandomForestClassifier(n_estimators=250, max_depth=12, random_state=42)\n","        model.fit(X, y)\n","    elif classifier == \"GBC\":\n","        from sklearn.ensemble import GradientBoostingClassifier\n","        model = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n","        model.fit(X, y)\n","\n","    print ('[INFO] Model training complete.')\n","    print ('[INFO] Training Accuracy: %.2f' %model.score(X, y))\n","    return model\n","\n","def test_model(X, y, model):\n","\n","    pred = model.predict(X)\n","    precision = metrics.precision_score(y, pred, average='weighted', labels=np.unique(pred))\n","    recall = metrics.recall_score(y, pred, average='weighted', labels=np.unique(pred))\n","    f1 = metrics.f1_score(y, pred, average='weighted', labels=np.unique(pred))\n","    accuracy = metrics.accuracy_score(y, pred)\n","\n","    print ('--------------------------------')\n","    print ('[RESULTS] Accuracy: %.2f' %accuracy)\n","    print ('[RESULTS] Precision: %.2f' %precision)\n","    print ('[RESULTS] Recall: %.2f' %recall)\n","    print ('[RESULTS] F1: %.2f' %f1)\n","    print ('--------------------------------')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SinNSsH9HYfe"},"outputs":[],"source":["import cv2\n","import numpy as np\n","import pylab as plt\n","from glob import glob\n","import argparse\n","import os\n","import pickle as pkl\n","import math\n","\n","def check_args(args):\n","\n","    if not os.path.exists(args.image_dir):\n","        raise ValueError(\"Image directory does not exist\")\n","\n","    if not os.path.exists(args.output_dir):\n","        raise ValueError(\"Output directory does not exist\")\n","\n","    return args\n","\n","def parse_args():\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\"-i\", \"--image_dir\" , help=\"Path to images\", required=True)\n","    parser.add_argument(\"-m\", \"--model_path\", help=\"Path to .p model\", required=True)\n","    parser.add_argument(\"-o\", \"--output_dir\", help=\"Path to output directory\", required = True)\n","    args = parser.parse_args()\n","    return check_args(args)\n","\n","def infer_create_features(img):\n","    \n","    rgb_img, full_img = get_int_rgb(img)\n","\n","    img_gray = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2GRAY)\n","\n","    features, _ = create_features(full_img, img_gray, label=None, train=False)\n","\n","    return features\n","\n","def compute_prediction(img, model):\n","\n","    border = 5 # (haralick neighbourhood - 1) / 2\n","    \n","    full_img = np.moveaxis(img,0,-1)\n","    \n","\n","    full_img = cv2.copyMakeBorder(full_img, top=border, bottom=border, \\\n","                                  left=border, right=border, \\\n","                                  borderType = cv2.BORDER_CONSTANT, \\\n","                                  value=[0]*3)\n","    \n","    img = np.moveaxis(full_img,-1,0)\n","\n","    features = infer_create_features(img)\n","    predictions = model.predict(features.reshape(-1, features.shape[1]))\n","    pred_size = int(math.sqrt(features.shape[0]))\n","    inference_img = predictions.reshape(pred_size, pred_size)\n","    print(inference_img.shape)\n","\n","    return inference_img\n","\n","def compute_prediction_prob(img, model):\n","\n","    border = 5 # (haralick neighbourhood - 1) / 2\n","    \n","    full_img = np.moveaxis(img,0,-1)\n","    \n","\n","    full_img = cv2.copyMakeBorder(full_img, top=border, bottom=border, \\\n","                                  left=border, right=border, \\\n","                                  borderType = cv2.BORDER_CONSTANT, \\\n","                                  value=[0]*3)\n","    \n","    img = np.moveaxis(full_img,-1,0)\n","\n","    features = infer_create_features(img)\n","    predictions = model.predict(features.reshape(-1, features.shape[1]))\n","    predictions_prob = model.predict_proba(features.reshape(-1, features.shape[1]))\n","#     print(predictions.shape)\n","    pred_size = int(math.sqrt(features.shape[0]))\n","    inference_img = predictions.reshape(pred_size, pred_size)\n","    prob_img = predictions_prob[:,1].reshape(pred_size, pred_size)\n","#     print(inference_img.shape)\n","\n","    return inference_img, prob_img\n","\n","def infer_images(image_dir, model_path, output_dir):\n","\n","    filelist = glob(os.path.join(image_dir,'*.jpg'))\n","\n","    print ('[INFO] Running inference on %s test images' %len(filelist))\n","\n","    model = pkl.load(open( model_path, \"rb\" ) )\n","\n","    for file in filelist:\n","        print ('[INFO] Processing images:', os.path.basename(file))\n","        inference_img = compute_prediction(cv2.imread(file, 1), model)\n","        cv2.imwrite(os.path.join(output_dir, os.path.basename(file)), inference_img)"]},{"cell_type":"markdown","metadata":{"id":"86XOkgAvHhyG"},"source":["##### Data Loader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SGxWe6-nMjbw"},"outputs":[],"source":["import numpy as np\n","import random\n","import torch\n","from torch import nn, optim\n","from tqdm.autonotebook import tqdm\n","from sklearn.metrics import accuracy_score\n","from torch.utils.data import DataLoader, random_split, SubsetRandomSampler\n","from torch.utils.tensorboard import SummaryWriter\n","import argparse\n","from sklearn.metrics import jaccard_score\n","\n","from data import create_dataset\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# prepare training and validation data loaders\n","data_all = create_dataset(\n","    datadir = '/content/drive/MyDrive/Colab Notebooks/smoke-plume-detection/data', # /path/to/image/data/\n","    seglabeldir = '/content/drive/MyDrive/Colab Notebooks/smoke-plume-detection/segmentation_labels', # /path/to/segmentation/labels/for/training/\n","    mult=1)\n","\n","len_all = len(data_all)\n","split_1 = (15*len_all) // 100\n","split_2 = 2*split_1\n","indices = list(range(len_all))\n","random.seed(9001)\n","random.shuffle(indices)\n","\n","test_sampler = SubsetRandomSampler(indices[:split_1])\n"]},{"cell_type":"markdown","metadata":{"id":"2mXs8BrNfqj7"},"source":["##### Plots"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"gv6LbAjDDACI"},"outputs":[],"source":["import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","from torch.utils.data import DataLoader, random_split, RandomSampler\n","from tqdm.autonotebook import tqdm\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import jaccard_score\n","\n","from network_ori import *\n","from model_unet import *\n","from modeling.deeplab import *\n","\n","np.random.seed(3)\n","torch.manual_seed(3)\n","\n","batch_size = 1 # 1 to create diagnostic images, any value otherwise\n","all_dl = DataLoader(data_all, batch_size=batch_size, sampler=test_sampler)\n","progress = tqdm(enumerate(all_dl), total=len(all_dl))\n","\n","# load model\n","device = torch.device('cpu')\n","\n","modelpaths={\n","    'u-net':'/content/drive/MyDrive/Colab Notebooks/smoke-plume-detection/models/unet_ep300_lr3e-01_bs60_mo0.7_111.model',\n","    'r2u-net':'/content/drive/MyDrive/Colab Notebooks/smoke-plume-detection/models/r2unet_ep300_lr5e-01_bs60_mo0.7_104.model',\n","    'attu-net':'/content/drive/MyDrive/Colab Notebooks/smoke-plume-detection/models/attunet_ep300_lr5e-01_bs60_mo0.7_116.model',\n","    'r2attu-net':'/content/drive/MyDrive/Colab Notebooks/smoke-plume-detection/models/r2attunet_ep300_lr5e-01_bs60_mo0.7_076.model',\n","    'dlabv3':'/content/drive/MyDrive/Colab Notebooks/smoke-plume-detection/models/dlabv3res_ep300_lr7e-01_bs60_mo0.7_147.model',\n","    # 'svm': '/content/drive/MyDrive/Colab Notebooks/smoke-plume-detection/models/svm.p',\n","    'rf': '/content/drive/MyDrive/Colab Notebooks/smoke-plume-detection/models/rf.p',\n","    # 'gbc':'/content/drive/MyDrive/Colab Notebooks/smoke-plume-detection/models/gbc.p'\n","    }\n","\n","\n","models = {\n","    'u-net': U_Net(img_ch=12, output_ch=1),\n","    'r2u-net': R2U_Net(img_ch=12, output_ch=1),\n","    'attu-net': AttU_Net(img_ch=12, output_ch=1),\n","    'r2attu-net': R2AttU_Net(img_ch=12, output_ch=1),\n","    'dlabv3': DeepLab(num_classes=1, backbone='resnet')\n","\n","}\n","\n","ml_models = {\n","    # 'svm': pkl.load(open(modelpaths['svm'], \"rb\" )),\n","    'rf': pkl.load(open(modelpaths['rf'],\"rb\")),\n","    # 'gbc':pkl.load(open(modelpaths['gbc'],\"rb\"))\n","}\n","\n","\n","for model in list(models.keys()):\n","    models[model].to(device)\n","    models[model].load_state_dict(torch.load(modelpaths[model],\n","                                                        map_location=device))\n","    models[model].eval()\n","\n","\n","# define loss function\n","loss_fn = nn.BCEWithLogitsLoss()\n","\n","# run through test data\n","\n","\n","\n","all_ious = {}\n","for model in list(models.keys()):\n","    all_ious[model] = []\n","all_accs = {}\n","for model in list(models.keys()):\n","    all_accs[model] = []\n","all_arearatios = {}\n","for model in list(models.keys()):\n","    all_arearatios[model] = []\n","\n","\n","for i, batch in progress:\n","    x, y = batch['img'].float().to(device), batch['fpt'].float().to(device)\n","    idx = batch['idx']\n","\n","    output = {}\n","    pred = {}\n","\n","    # create plot\n","    f, axs = plt.subplots(4, 6, figsize=(6, 4))\n","\n","    i = 0\n","\n","    for model in list(models.keys()):\n","        output = models[model](x)\n","\n","        # obtain binary prediction map\n","        pred = np.zeros(output.shape)\n","        pred[output\u003e=0] =1\n","\n","        # derive Iou score\n","        cropped_iou = []\n","        for j in range(y.shape[0]):\n","            z = jaccard_score(y[j].flatten().detach().numpy(),\n","                            pred[j][0].flatten())\n","            if (np.sum(pred[j][0]) != 0 and\n","                np.sum(y[j].detach().numpy()) != 0):\n","                cropped_iou.append(z)\n","\n","        all_ious[model] = [*all_ious[model], *cropped_iou]\n","\n","    \n","        # derive scalar binary labels on a per-image basis\n","        y_bin = np.array(np.sum(y.detach().numpy(),\n","                                axis=(1,2)) != 0).astype(int)\n","        prediction = np.array(np.sum(pred,\n","                                axis=(1,2,3)) != 0).astype(int)\n","\n","        # derive image-wise accuracy for this batch\n","        all_accs[model].append(accuracy_score(y_bin, prediction))\n","\n","        # derive binary segmentation map from prediction\n","        output_binary = np.zeros(output.shape)\n","        output_binary[output.cpu().detach().numpy() \u003e= 0] = 1\n","\n","        # derive smoke areas\n","        area_pred = np.sum(output_binary, axis=(1,2,3))\n","        area_true = np.sum(y.cpu().detach().numpy(), axis=(1,2))\n","\n","        # derive smoke area ratios\n","        arearatios = []\n","        for k in range(len(area_pred)):\n","            if area_pred[k] == 0 and area_true[k] == 0:\n","                arearatios.append(1)\n","            elif area_true[k] == 0:\n","                arearatios.append(0)\n","            else:\n","                arearatios.append(area_pred[k]/area_true[k])\n","        all_arearatios[model] = np.ravel([*all_arearatios[model], *arearatios])\n","\n","\n","        if batch_size == 1:\n","\n","            if prediction == 1 and y_bin == 1:\n","                res = 'true_pos'\n","            elif prediction == 0 and y_bin == 0:\n","                res = 'true_neg'\n","            elif prediction == 0 and y_bin == 1:\n","                res = 'false_neg'\n","            elif prediction == 1 and y_bin == 0:\n","                res = 'false_pos'\n","\n","\n","        # RGB plot\n","        axs[0,i].imshow(0.2+1.5*(np.dstack([x[0][3], x[0][2], x[0][1]])-\n","                    np.min([x[0][3].numpy(),\n","                            x[0][2].numpy(),\n","                            x[0][1].numpy()]))/\n","                   (np.max([x[0][3].numpy(),\n","                            x[0][2].numpy(),\n","                            x[0][1].numpy()])-\n","                    np.min([x[0][3].numpy(),\n","                            x[0][2].numpy(),\n","                            x[0][1].numpy()])),\n","                   origin='upper')\n","        axs[0,i].set_title({'true_pos': 'True Positive',\n","                       'true_neg': 'True Negative',\n","                       'false_pos': 'False Positive',\n","                       'false_neg': 'False Negative'}[res],\n","                      fontsize=8)\n","        axs[0,i].set_xticks([])\n","        axs[0,i].set_yticks([])\n","\n","        # false color plot\n","        axs[1,i].imshow(0.2+(np.dstack([x[0][0], x[0][9], x[0][10]])-\n","                    np.min([x[0][0].numpy(),\n","                            x[0][9].numpy(),\n","                            x[0][10].numpy()]))/\n","                   (np.max([x[0][0].numpy(),\n","                            x[0][9].numpy(),\n","                            x[0][10].numpy()])-\n","                    np.min([x[0][0].numpy(),\n","                            x[0][9].numpy(),\n","                            x[0][10].numpy()])),\n","                   origin='upper')\n","\n","        axs[1,i].set_xticks([])\n","        axs[1,i].set_yticks([])\n","\n","        # segmentation ground-truth and prediction\n","        axs[2,i].imshow(y[0], cmap='Reds', alpha=0.3)\n","        axs[2,i].imshow(pred[0][0], cmap='Greens', alpha=0.3)\n","        axs[2,i].set_xticks([])\n","        axs[2,i].set_yticks([])\n","\n","        this_iou = jaccard_score(y[0].flatten().detach().numpy(),\n","                                 pred[0][0].flatten())\n","        axs[2,i].annotate(\"IoU={:.2f}\".format(this_iou), xy=(5,15), fontsize=8)\n","\n","        #segmentation probability distribution\n","       \n","        prob = torch.sigmoid(output)  #get the probability distribution\n","        axs[3,i].imshow(prob[0][0].detach().numpy(), cmap = 'hot', vmin=0, vmax=1)\n","        axs[3,i].set_xticks([])\n","        axs[3,i].set_yticks([])\n","        axs[3,i].set_xlabel(model)\n","        # axs[3,i].annotate(\"Probability@\"+ model, xy=(5,15))\n","        # print(prob[0][0].size())\n","        ##\n","        i+=1\n","\n","    for model in ml_models.keys():\n","\n","         # RGB plot\n","        axs[0,i].imshow(0.2+1.5*(np.dstack([x[0][3], x[0][2], x[0][1]])-\n","                    np.min([x[0][3].numpy(),\n","                            x[0][2].numpy(),\n","                            x[0][1].numpy()]))/\n","                   (np.max([x[0][3].numpy(),\n","                            x[0][2].numpy(),\n","                            x[0][1].numpy()])-\n","                    np.min([x[0][3].numpy(),\n","                            x[0][2].numpy(),\n","                            x[0][1].numpy()])),\n","                   origin='upper')\n","        axs[0,i].set_title({'true_pos': 'True Positive',\n","                       'true_neg': 'True Negative',\n","                       'false_pos': 'False Positive',\n","                       'false_neg': 'False Negative'}[res],\n","                      fontsize=8)\n","        axs[0,i].set_xticks([])\n","        axs[0,i].set_yticks([])\n","\n","        # false color plot\n","        axs[1,i].imshow(0.2+(np.dstack([x[0][0], x[0][9], x[0][10]])-\n","                    np.min([x[0][0].numpy(),\n","                            x[0][9].numpy(),\n","                            x[0][10].numpy()]))/\n","                   (np.max([x[0][0].numpy(),\n","                            x[0][9].numpy(),\n","                            x[0][10].numpy()])-\n","                    np.min([x[0][0].numpy(),\n","                            x[0][9].numpy(),\n","                            x[0][10].numpy()])),\n","                   origin='upper')\n","\n","        axs[1,i].set_xticks([])\n","        axs[1,i].set_yticks([])\n","\n","\n","        # segmentation ground-truth and prediction\n","\n","        pred, prob = compute_prediction_prob(x[0].numpy(), ml_models[model])\n","\n","        axs[2,i].imshow(y[0], cmap='Reds', alpha=0.3)\n","        axs[2,i].imshow(pred, cmap='Greens', alpha=0.3)\n","        axs[2,i].set_xticks([])\n","        axs[2,i].set_yticks([])\n","\n","        this_iou = jaccard_score(y[0].flatten().detach().numpy(),\n","                                 pred.flatten())\n","        axs[2,i].annotate(\"IoU={:.2f}\".format(this_iou), xy=(5,15), fontsize=8)\n","\n","        #prob\n","        axs[3,i].imshow(prob, cmap='hot', vmin=0, vmax=1)\n","        axs[3,i].set_xticks([])\n","        axs[3,i].set_yticks([])\n","        axs[3,i].set_xlabel(model)\n","\n","        i+=1\n","\n","\n","    # f.subplots_adjust(0.05, 0.02, 0.95, 0.9, 0.05, 0.05)\n","\n","    plt.savefig('./eval-results/union/'+res+(os.path.split(batch['imgfile'][0])[1]).\\\n","                replace('.tif', '_eval.png').replace(':', '_'),\n","                tight_layout=True, dpi=200)\n","    plt.close()\n","\n","# print('iou:', len(all_ious), np.average(all_ious))\n","# print('accuracy:', len(all_accs), np.average(all_accs))\n","# print('mean area ratio:', len(all_arearatios), np.average(all_arearatios),\n","#       np.std(all_arearatios)/np.sqrt(len(all_arearatios)-1))"]},{"cell_type":"markdown","metadata":{"id":"DG_Dgmf7d5pB"},"source":["##### paper-ready"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":727},"executionInfo":{"elapsed":783254,"status":"error","timestamp":1672842504058,"user":{"displayName":"Jiantao Wu","userId":"07523577744578165226"},"user_tz":-480},"id":"9eVrXmDXd_gN","outputId":"8bf3a72b-0044-4e9a-f317-6949737d7550"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d501f524f3cd42d4bb9ae48ceb4a4fe7","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/428 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","[                                                                        ] N/A%"]},{"name":"stdout","output_type":"stream","text":["[INFO] Computing local binary pattern features.\n","[INFO] Computing haralick features.\n"]},{"name":"stderr","output_type":"stream","text":["[========================================================================] 100%WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","[                                                                        ] N/A%"]},{"name":"stdout","output_type":"stream","text":["[INFO] Computing local binary pattern features.\n","[INFO] Computing haralick features.\n"]},{"name":"stderr","output_type":"stream","text":["[========================================================================] 100%WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","[                                                                        ] N/A%"]},{"name":"stdout","output_type":"stream","text":["[INFO] Computing local binary pattern features.\n","[INFO] Computing haralick features.\n"]},{"name":"stderr","output_type":"stream","text":["[========================================================================] 100%WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","[                                                                        ] N/A%"]},{"name":"stdout","output_type":"stream","text":["[INFO] Computing local binary pattern features.\n","[INFO] Computing haralick features.\n"]},{"name":"stderr","output_type":"stream","text":["[========================================================================] 100%"]},{"name":"stdout","output_type":"stream","text":["Saving the plot...\n","Plot saved..\n"]},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-49-ea3f8d170df9\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 93\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'img'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fpt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'idx'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 259\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1195\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 671\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m\u003clistcomp\u003e\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Colab Notebooks/smoke-plume-detection/IndustrialSmokePlumeDetection/segmentation/data.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;31m# read in image data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 154\u001b[0;31m         \u001b[0mimgfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         imgdata = np.array([imgfile.read(i) for i in\n\u001b[1;32m    156\u001b[0m                             [1,2,3,4,5,6,7,8,9,10,12,13]])\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/rasterio/env.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0menv_ctor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 444\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/rasterio/__init__.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, driver, width, height, count, crs, transform, dtype, nodata, sharing, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 304\u001b[0;31m             \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatasetReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msharing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msharing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"r+\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             dataset = get_writer_for_path(path, driver=driver)(\n","\u001b[0;32mrasterio/_base.pyx\u001b[0m in \u001b[0;36mrasterio._base.DatasetBase.__init__\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/rasterio/_path.py\u001b[0m in \u001b[0;36mname\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 113\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;34m\"\"\"The unparsed path's original path\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","from torch.utils.data import DataLoader, random_split, RandomSampler\n","from tqdm.autonotebook import tqdm\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import jaccard_score\n","\n","from network_ori import *\n","from model_unet import *\n","from modeling.deeplab import *\n","\n","np.random.seed(3)\n","torch.manual_seed(3)\n","\n","batch_size = 1 # 1 to create diagnostic images, any value otherwise\n","all_dl = DataLoader(data_all, batch_size=batch_size, sampler=test_sampler)\n","progress = tqdm(enumerate(all_dl), total=len(all_dl))\n","\n","# load model\n","device = torch.device('cpu')\n","\n","wanted = ['212846_2019-03-02T11_08_59.015Z_6_eval.png', '74662_2019-12-05T10_16_17.136Z_40_eval.png', '46361_2019-10-06T10_16_24.037Z_12_eval.png',\n","          '46361_2019-06-23T10_16_27.346Z_7_eval.png', '11173_2019-04-23T10_02_53.879Z_0_eval.png', '10362_2019-12-10T10_16_43.686Z_0_eval.png', \n","          '6594_2019-03-03T10_28_32.286Z_3_eval.png', '198_2019-06-30T10_06_44.463Z_22_eval.png']\n","# wanted = wanted[:2]\n","\n","wanted = ['74662_2019-12-05T10_16_17.136Z_40_eval.png', '46361_2019-06-23T10_16_27.346Z_7_eval.png', '10362_2019-12-10T10_16_43.686Z_0_eval.png', \n","          '6594_2019-03-03T10_28_32.286Z_3_eval.png']\n","\n","modelpaths={\n","    'U-Net':'/content/drive/MyDrive/Colab Notebooks/smoke-plume-detection/models/unet_ep300_lr3e-01_bs60_mo0.7_111.model',\n","    'R2U-Net':'/content/drive/MyDrive/Colab Notebooks/smoke-plume-detection/models/r2unet_ep300_lr5e-01_bs60_mo0.7_104.model',\n","    'AttU-Net':'/content/drive/MyDrive/Colab Notebooks/smoke-plume-detection/models/attunet_ep300_lr5e-01_bs60_mo0.7_116.model',\n","    # 'R2AttU-Net':'/content/drive/MyDrive/Colab Notebooks/smoke-plume-detection/models/r2attunet_ep300_lr5e-01_bs60_mo0.7_076.model',\n","    'DLabv3+':'/content/drive/MyDrive/Colab Notebooks/smoke-plume-detection/models/dlabv3res_ep300_lr7e-01_bs60_mo0.7_147.model',\n","    # 'SVM': '/content/drive/MyDrive/Colab Notebooks/smoke-plume-detection/models/svm.p',\n","    'RF': '/content/drive/MyDrive/Colab Notebooks/smoke-plume-detection/models/rf.p',\n","    # 'GBC':'/content/drive/MyDrive/Colab Notebooks/smoke-plume-detection/models/gbc.p'\n","    }\n","\n","\n","models = {\n","    'U-Net': U_Net(img_ch=12, output_ch=1),\n","    'R2U-Net': R2U_Net(img_ch=12, output_ch=1),\n","    'AttU-Net': AttU_Net(img_ch=12, output_ch=1),\n","    # 'R2AttU-Net': R2AttU_Net(img_ch=12, output_ch=1),\n","    'DLabv3+': DeepLab(num_classes=1, backbone='resnet')\n","\n","}\n","\n","ml_models = {\n","    # 'SVM': pkl.load(open(modelpaths['SVM'], \"rb\" )),\n","    'RF': pkl.load(open(modelpaths['RF'],\"rb\")),\n","    # 'GBC':pkl.load(open(modelpaths['GBC'],\"rb\"))\n","}\n","\n","\n","for model in list(models.keys()):\n","    models[model].to(device)\n","    models[model].load_state_dict(torch.load(modelpaths[model],\n","                                                        map_location=device))\n","    models[model].eval()\n","\n","\n","# define loss function\n","loss_fn = nn.BCEWithLogitsLoss()\n","\n","# run through test data\n","\n","\n","\n","all_ious = {}\n","for model in list(models.keys()):\n","    all_ious[model] = []\n","all_accs = {}\n","for model in list(models.keys()):\n","    all_accs[model] = []\n","all_arearatios = {}\n","for model in list(models.keys()):\n","    all_arearatios[model] = []\n","\n","\n","count = 0\n","\n","# create plot\n","f, axs = plt.subplots(len(wanted), 7, figsize=(7, len(wanted)))\n","\n","\n","\n","i = 0 \n","for k, batch in progress:\n","    x, y = batch['img'].float().to(device), batch['fpt'].float().to(device)\n","    idx = batch['idx']\n","\n","    output = {}\n","    pred = {}\n","\n","    imgname = (os.path.split(batch['imgfile'][0])[1]).replace('.tif', '_eval.png').replace(':', '_')\n","    if imgname in wanted and count\u003clen(wanted):\n","\n","        # RGB plot\n","        axs[i,0].imshow(0.2+1.5*(np.dstack([x[0][3], x[0][2], x[0][1]])-\n","                    np.min([x[0][3].numpy(),\n","                            x[0][2].numpy(),\n","                            x[0][1].numpy()]))/\n","                (np.max([x[0][3].numpy(),\n","                            x[0][2].numpy(),\n","                            x[0][1].numpy()])-\n","                    np.min([x[0][3].numpy(),\n","                            x[0][2].numpy(),\n","                            x[0][1].numpy()])),\n","                origin='upper')\n","        axs[i,0].set_xticks([])\n","        axs[i,0].set_yticks([])\n","        if i == len(wanted)-1:\n","            axs[i,0].set_xlabel('RGB')\n","        \n","        # Ground truth\n","        axs[i,1].imshow(y[0], cmap='Reds', alpha=0.3)\n","        axs[i,1].set_xticks([])\n","        axs[i,1].set_yticks([])\n","        if i == len(wanted)-1:\n","            axs[i,1].set_xlabel('Gound-truth')\n","\n","        j = 2\n","        for model in list(models.keys()):\n","            \n","            output = models[model](x)\n","\n","            # obtain binary prediction map\n","            pred = np.zeros(output.shape)\n","            pred[output\u003e=0] =1\n","\n","            \n","             # derive scalar binary labels on a per-image basis\n","            y_bin = np.array(np.sum(y.detach().numpy(),\n","                                    axis=(1,2)) != 0).astype(int)\n","            prediction = np.array(np.sum(pred,\n","                                    axis=(1,2,3)) != 0).astype(int)\n","            \n","            if batch_size == 1:\n","\n","                if prediction == 1 and y_bin == 1:\n","                    res = '++'\n","                elif prediction == 0 and y_bin == 0:\n","                    res = '+-'\n","                elif prediction == 0 and y_bin == 1:\n","                    res = '--'\n","                elif prediction == 1 and y_bin == 0:\n","                    res = '-+'\n","            \n","            #prob plot\n","       \n","            prob = torch.sigmoid(output)  #get the probability distribution\n","            this_iou = jaccard_score(y[0].flatten().detach().numpy(),\n","                                    pred[0][0].flatten())\n","            \n","            axs[i,j].imshow(prob[0][0].detach().numpy(), cmap = 'hot', vmin=0, vmax=1)\n","            axs[i,j].text(5,15, res+\"IoU={:.2f}\".format(this_iou), fontsize=6, color='white')\n","            axs[i,j].set_xticks([])\n","            axs[i,j].set_yticks([])\n","\n","            if i == len(wanted)-1:\n","               axs[i,j].set_xlabel(model)\n","\n","            j+=1\n","\n","        \n","        for model in ml_models.keys():\n","            \n","            pred, prob = compute_prediction_prob(x[0].numpy(), ml_models[model])\n","            this_iou = jaccard_score(y[0].flatten().detach().numpy(),\n","                                 pred.flatten())\n","            \n","            # derive scalar binary labels on a per-image basis\n","            y_bin = np.array(np.sum(y.detach().numpy(),\n","                                    axis=(1,2)) != 0).astype(int)\n","            prediction = np.array(np.sum(pred,\n","                                    axis=(0,1)) != 0).astype(int)\n","\n","            if batch_size == 1:\n","\n","                if prediction == 1 and y_bin == 1:\n","                    res = '++'\n","                elif prediction == 0 and y_bin == 0:\n","                    res = '+-'\n","                elif prediction == 0 and y_bin == 1:\n","                    res = '--'\n","                elif prediction == 1 and y_bin == 0:\n","                    res = '-+'\n","\n","            #prob plot\n","            im = axs[i,j].imshow(prob, cmap='hot', vmin=0, vmax=1) #im for colorbar\n","            axs[i,j].text(5,15,res+\"IoU={:.2f}\".format(this_iou), fontsize=6, color='white')\n","            axs[i,j].set_xticks([])\n","            axs[i,j].set_yticks([])\n","\n","            if i == len(wanted)-1:\n","               axs[i,j].set_xlabel(model)\n","            \n","            j+=1\n","\n","        \n","        if i == len(wanted)-1:\n","            print('Saving the plot...')\n","            f.subplots_adjust(left=0.05, right=0.85, top=0.9, bottom = 0.1, wspace=0.05, hspace=0.05)\n","            cb_ax = f.add_axes([0.87, 0.1, 0.06, 0.8])\n","            cbar = f.colorbar(im, cax=cb_ax)\n","            plt.savefig('./eval-results/paper-ready/results.png', tight_layout=True, dpi=800)\n","            plt.close()\n","            print('Plot saved..')\n","\n","        i+=1\n","\n","    continue\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":937,"status":"ok","timestamp":1672840958009,"user":{"displayName":"Jiantao Wu","userId":"07523577744578165226"},"user_tz":-480},"id":"fQ-Soe8FF0Bq","outputId":"59d64b82-55f6-4604-d123-8d6bc79d5b86"},"outputs":[{"name":"stdout","output_type":"stream","text":["data\t\t\t\t    README.md\n","dl-eval.ipynb\t\t\t    runs\n","ep300_lr7e-01_bs60_mo0.7_000.model  segmentation_labels\n","eval-results\t\t\t    segmentation_labels.tar.gz\n","images.tar.gz\t\t\t    smoke-ml-bal.ipynb\n","IndustrialSmokePlumeDetection\t    smoke-plume-detection.ipynb\n","models\t\t\t\t    test_labels\n","pytorch-deeplab-xception\t    test.tif\n","pytorch-deeplab-xception-bacup\n"]}],"source":["!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SEgyjhizB2Nf"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","device = torch.device('cpu')\n","\n","batch_size = 1 # 1 to create diagnostic images, any value otherwise\n","all_dl = DataLoader(data_all, batch_size=batch_size, sampler=test_sampler)\n","progress = tqdm(enumerate(all_dl), total=len(all_dl))\n","\n","for i, batch in progress:\n","    x = batch['img'].float().to(device)\n","    y = batch['fpt'].float().to(device)\n","\n","    for j in range(x.shape[0]):\n","        # create plot\n","        f, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(1, 3))\n","\n","        # RGB\n","        img =  (np.dstack([x[j][3], x[j][2], x[j][1]])-\n","                        np.min([x[j][3].numpy(),\n","                                x[j][2].numpy(),\n","                                x[j][1].numpy()]))/ \\\n","                    (np.max([x[j][3].numpy(),\n","                                x[j][2].numpy(),\n","                                x[j][1].numpy()])-\n","                        np.min([x[j][3].numpy(),\n","                                x[j][2].numpy(),\n","                                x[j][1].numpy()]))\n","        ax1.imshow(0.2+1.5*img) # more brighten\n","        ax1.set_xticks([])\n","        ax1.set_yticks([])\n","        \n","        \n","\n","        #ground-truth and pred\n","        ax2.imshow(y[j], cmap='Reds', alpha=0.3)\n","        ax2.imshow(compute_prediction((img * 255).round().astype(np.uint8), model), cmap='Greens', alpha=0.3)\n","        ax2.set_xticks([])\n","        ax2.set_yticks([])\n","        \n","        #only ground-truth\n","        ax3.imshow(y[j], cmap='Reds', alpha=0.3)\n","        ax3.set_xticks([])\n","        ax3.set_yticks([])\n","        \n","        \n","        f.subplots_adjust(0.05, 0.02, 0.95, 0.9, 0.05, 0.05)\n","\n","        plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":364,"status":"ok","timestamp":1672762293277,"user":{"displayName":"Jiantao Wu","userId":"07523577744578165226"},"user_tz":-480},"id":"e6V6W3-vA4DF","outputId":"93532472-0564-48c7-cf58-f6fd3fa5403d"},"outputs":[{"name":"stdout","output_type":"stream","text":["drive  sample_data\n"]}],"source":["!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":339,"status":"ok","timestamp":1672843664739,"user":{"displayName":"Jiantao Wu","userId":"07523577744578165226"},"user_tz":-480},"id":"v-j9r7bkTJgz","outputId":"a7de873b-5462-4154-9acc-6caa59b04bb7"},"outputs":[{"name":"stdout","output_type":"stream","text":["[5, 2, 7, 1, 8, 4, 3, 6, 0, 9]\n"]}],"source":["random.seed(10)\n","l = list(range(10))\n","random.shuffle(l)\n","print(l)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r7Het_7JPyFc"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyO12A5ACMh8qj/Fl3G6W+Oz","name":"","toc_visible":true,"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"02d4fc9dc1374b6d807a87fffe042e9b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0359a1cf79934ae9bb6d6d2fa96dba25":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0c67d31302a247ccb3dcddcc264c6680":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"124425d01fe348a7b0ca70bbce1dbf52":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e03a54635cf4a65a7eb127e6a4d2ef3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"260812fc24074fad88c25b81bb044321":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f3d2fc6305a74a91bc275324efd53f02","IPY_MODEL_379c2497c66b44eba54daec4afc9eaa2","IPY_MODEL_ee5d10c64f264ef6ab678908ac05f549"],"layout":"IPY_MODEL_1e03a54635cf4a65a7eb127e6a4d2ef3"}},"264f4cf69cb54538b0909fab3e182374":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e586c4e118434147b6fcd5ade77b7d40","placeholder":"​","style":"IPY_MODEL_35642e2a0ff24cc497966ef4e6dea3be","value":" 283/428 [13:03\u0026lt;05:52,  2.43s/it]"}},"2722e9ef08b84aa5a681013799425ec8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2dcb5865a6dc485aa958b157acb42639":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b236f612048e451cb5459d8a81413487","IPY_MODEL_aab32b50c4284724847649f39f3d7b59","IPY_MODEL_56e1354b16fe48169fd748b249f0a231"],"layout":"IPY_MODEL_ae8bac8056a7492c90e126532b7a3670"}},"35642e2a0ff24cc497966ef4e6dea3be":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"369241a4e7a14db9b57165d8e609fffc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"379c2497c66b44eba54daec4afc9eaa2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_ffcfd3f0f67f4937bd49b28d57badad3","max":428,"min":0,"orientation":"horizontal","style":"IPY_MODEL_77200fdca2eb4b65b74c05479aea8fbb","value":6}},"4ef0970a39ee40e19df4c60dd9dfbf84":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"56e1354b16fe48169fd748b249f0a231":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ef0970a39ee40e19df4c60dd9dfbf84","placeholder":"​","style":"IPY_MODEL_369241a4e7a14db9b57165d8e609fffc","value":" 361/428 [18:39\u0026lt;06:51,  6.14s/it]"}},"6986061a887b4fac874d2809d0369457":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"77200fdca2eb4b65b74c05479aea8fbb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"831502d5e86543209934d47562f42622":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e1cfedc46c8481b9947466d1d2ebabe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a0e168985ab540f0b537282d2360b5a4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a52961075975478ba343cf91c62df847":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aab32b50c4284724847649f39f3d7b59":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_0c67d31302a247ccb3dcddcc264c6680","max":428,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0359a1cf79934ae9bb6d6d2fa96dba25","value":362}},"ae8bac8056a7492c90e126532b7a3670":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af3c449c45c8433e95bacbd41860fb47":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b236f612048e451cb5459d8a81413487":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c4b6c79733bb406384dd7d6e07305939","placeholder":"​","style":"IPY_MODEL_a52961075975478ba343cf91c62df847","value":" 58%"}},"c3ebe44de2314845b1eaf944784f0eb9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_124425d01fe348a7b0ca70bbce1dbf52","placeholder":"​","style":"IPY_MODEL_af3c449c45c8433e95bacbd41860fb47","value":" 66%"}},"c4b6c79733bb406384dd7d6e07305939":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d098ea7ba8c14ea7a59a1ce7bb0da1a8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d501f524f3cd42d4bb9ae48ceb4a4fe7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c3ebe44de2314845b1eaf944784f0eb9","IPY_MODEL_ef2de6a737404f05aaeb6efb985b1f5f","IPY_MODEL_264f4cf69cb54538b0909fab3e182374"],"layout":"IPY_MODEL_d098ea7ba8c14ea7a59a1ce7bb0da1a8"}},"e586c4e118434147b6fcd5ade77b7d40":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee5d10c64f264ef6ab678908ac05f549":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_831502d5e86543209934d47562f42622","placeholder":"​","style":"IPY_MODEL_2722e9ef08b84aa5a681013799425ec8","value":" 6/428 [00:39\u0026lt;34:49,  4.95s/it]"}},"ef2de6a737404f05aaeb6efb985b1f5f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_02d4fc9dc1374b6d807a87fffe042e9b","max":428,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6986061a887b4fac874d2809d0369457","value":283}},"f3d2fc6305a74a91bc275324efd53f02":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a0e168985ab540f0b537282d2360b5a4","placeholder":"​","style":"IPY_MODEL_8e1cfedc46c8481b9947466d1d2ebabe","value":"  1%"}},"ffcfd3f0f67f4937bd49b28d57badad3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}